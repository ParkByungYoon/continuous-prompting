{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from model.continuous_prompt import ContinuousPromptingLLM\n",
    "from model.graph_encoder import GraphContinuousPromptModel\n",
    "from model.projection import BasicProjection\n",
    "from dataset import GraphDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from util import convert_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='test'\n",
    "TASK='cycle_check'\n",
    "MODEL_NAME = 'gin'\n",
    "SAVE_DIR=f'/home/bonbak/continuous-prompting/output/{TASK}'\n",
    "TASKS_DIR = f'/home/bonbak/continuous-prompting/task/{TASK}'\n",
    "DEVICE='cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G describes a graph among nodes 0, 1, 2, 3, 4, 5, 6, and 7.\n",
      "The edges in G are: (0, 1) (0, 5) (0, 6) (2, 6) (4, 7).\n",
      "You must answer with \"Yes\" or \"No\" under the question.\n",
      "Q: Is node 0 connected to node 2?\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "test_dataset = GraphDataset(f\"{TASKS_DIR}/{MODE}.jsonl\")\n",
    "print('\\n'.join((test_dataset.data[0]['node_information'], test_dataset.data[0]['edge_information'], test_dataset.data[0]['question'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data.dataset.GraphDataset at 0x76f0306364d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d3dbf3ffed4e159445e5d1f12e80aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ContinuousPromptingLLM(\n",
       "  (llm_model): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-17): 18 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "  )\n",
       "  (continuous_prompt_model): GraphContinuousPromptModel(\n",
       "    (model): GIN(\n",
       "      (convs): ModuleList(\n",
       "        (0): GINConv(nn=Sequential(\n",
       "          (0): Linear(in_features=5, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        ))\n",
       "        (1-2): 2 x GINConv(nn=Sequential(\n",
       "          (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "        ))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection_module): Linear(in_features=512, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_prompt_model = GraphContinuousPromptModel(input_dim=5, hidden_dim=512)\n",
    "projection_module = BasicProjection(continuous_prompt_model.model.hidden_dim)\n",
    "\n",
    "model = ContinuousPromptingLLM(\n",
    "    \"google/gemma-2b-it\",\n",
    "    continuous_prompt_model, \n",
    "    continuous_prompt_model.model.hidden_dim\n",
    ")\n",
    "\n",
    "model.continuous_prompt_model.load_state_dict(torch.load(f'{SAVE_DIR}/model/{MODEL_NAME}-encoder.bin'))\n",
    "model.projection_module.load_state_dict(torch.load(f'{SAVE_DIR}/model/{MODEL_NAME}-projection.bin'))\n",
    "\n",
    "continuous_prompt_model.to(DEVICE)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:35<00:00, 19.91it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = []\n",
    "label = []\n",
    "\n",
    "for input_text, continuous_prompt_input, answer_list in tqdm(test_dataset):\n",
    "    with torch.no_grad():\n",
    "        inputs_embeds, attention_mask = model.make_input_embed([input_text], continuous_prompt_input, embedding_first=True)\n",
    "        output = model.llm_model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, max_new_tokens=4)\n",
    "        pred.append(model.llm_tokenizer.batch_decode(output, skip_special_tokens=True)[0])\n",
    "        label.append(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, missed = convert_answer(pred)\n",
    "y_true, _ = convert_answer(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7814285714285715\n",
      "0.7171903881700554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
