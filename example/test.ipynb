{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from model.continuous_prompt import ContinuousPromptingLLM\n",
    "from model.text_encoder import TextContinuousPromptModel\n",
    "from model.projection import BasicProjection\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from dataset import Dataset\n",
    "from util import convert_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='test'\n",
    "TASK='cycle_check'\n",
    "MODEL_NAME = 'bert'\n",
    "SAVE_DIR=f'/home/bonbak/GraphToken/output/{TASK}'\n",
    "TASKS_DIR = f'/home/bonbak/GraphToken/task/{TASK}'\n",
    "DEVICE='cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G describes a graph among nodes 0, 1, 2, 3, 4, 5, 6, and 7.\n",
      "The edges in G are: (0, 1) (0, 5) (0, 6) (2, 6) (4, 7).\n",
      "You must answer with \"Yes\" or \"No\" under the question.\n",
      "Q: Is there a cycle in this graph?\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset(f\"{TASKS_DIR}/{MODE}.jsonl\")\n",
    "print('\\n'.join((test_dataset.data[0]['node_information'], test_dataset.data[0]['edge_information'], test_dataset.data[0]['question'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e94c8d07c841a09b5904d0a2a5e516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [01:21<00:00,  8.57it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = []\n",
    "label = []\n",
    "\n",
    "for input_text, continuous_prompt_input, answer_list in tqdm(test_dataset):\n",
    "    input_text = '\\n'.join((continuous_prompt_input['input_text_list'], input_text))\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=4)\n",
    "    answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    answer = answer.replace(input_text, '')\n",
    "\n",
    "    pred.append(answer)\n",
    "    label.append(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, missed = convert_answer(pred)\n",
    "y_true, _ = convert_answer(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6585714285714286\n",
      "0.7941429801894918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphToken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
