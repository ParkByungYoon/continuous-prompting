{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from model.continuous_prompt import ContinuousPromptingLLM\n",
    "from model.recsys_encoder import RecsysContinuousPromptModel\n",
    "from model.projection import BasicProjection\n",
    "from dataset import RecsysDataset\n",
    "\n",
    "from util import plot_and_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE='train'\n",
    "TASK='recommendation'\n",
    "MODEL_NAME = 'light-gcn'\n",
    "LLM_DIR = \"/SSL_NAS/bonbak/model/models--yanolja--EEVE-Korean-Instruct-2.8B-v1.0/snapshots/482db2d0ba911253d09342c34d0e42ac871bfea3\"\n",
    "SAVE_DIR=f'/home/bonbak/continuous-prompting/output/{TASK}'\n",
    "TASKS_DIR = f'/home/bonbak/continuous-prompting/task/{TASK}'\n",
    "DEVICE='cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RecsysDataset(f\"{TASKS_DIR}/{MODE}.jsonl\", f\"{TASKS_DIR}/edge.csv\")\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "num_users, num_items = len(train_dataset.user_mapping), len(train_dataset.item_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_prompt_model = RecsysContinuousPromptModel(num_users, num_items,f'{TASKS_DIR}/train_edge_index.pt')\n",
    "projection_module = BasicProjection(continuous_prompt_model.model.embedding_dim)\n",
    "\n",
    "model = ContinuousPromptingLLM(\n",
    "    LLM_DIR,\n",
    "    continuous_prompt_model, \n",
    "    continuous_prompt_model.model.embedding_dim\n",
    ")\n",
    "\n",
    "model.continuous_prompt_model.model.load_state_dict(torch.load(f'{SAVE_DIR}/model/{MODEL_NAME}.bin'))\n",
    "\n",
    "continuous_prompt_model.to(DEVICE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.projection_module.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.continuous_prompt_model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "c = 0\n",
    "loss_log_list = []\n",
    "min_loss = 1000000\n",
    "accumulate_step = 8\n",
    "\n",
    "def mean(l):\n",
    "    return sum(l)/len(l)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for input_text, continuous_prompt_input, answer_list in train_dataloader:\n",
    "        inputs_embeds, attention_mask, labels = model.make_seq2seq_input_label(input_text,continuous_prompt_input,answer_list, embedding_first=True)\n",
    "\n",
    "        generated_output = model.llm_model.forward(\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    attention_mask = attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "        generated_output.loss.backward()\n",
    "        \n",
    "        if c % accumulate_step == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        loss_log_list.append(generated_output.loss.item())\n",
    "        \n",
    "        if c % 80 == 0 and c!=0:\n",
    "            cur_loss = mean(loss_log_list[-accumulate_step:])\n",
    "            if min_loss > cur_loss:\n",
    "                model.eval()\n",
    "                model.to('cpu')\n",
    "                min_loss = cur_loss\n",
    "                torch.save(model.projection_module.state_dict(), f'{SAVE_DIR}/model/{MODEL_NAME}-projection.bin')\n",
    "                torch.save(model.continuous_prompt_model.state_dict(), f'{SAVE_DIR}/model/{MODEL_NAME}-encoder.bin')\n",
    "\n",
    "                inputs_embeds, attention_mask = model.make_input_embed(input_text,continuous_prompt_input, embedding_first=True)\n",
    "                output = model.llm_model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, pad_token_id=model.llm_tokenizer.eos_token_id, max_new_tokens=1)\n",
    "                print(input_text[0], model.llm_tokenizer.decode(output[0]))\n",
    "                plot_and_save(loss_log_list, f'{SAVE_DIR}/loss/{MODEL_NAME}.png')\n",
    "\n",
    "                model.train()\n",
    "                model.to(DEVICE)\n",
    "\n",
    "            print(f'step {c} | cur_loss : {cur_loss:.4f} | min_loss : {min_loss:.4f} ')\n",
    "        c+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
